
# Can complaints predict crime?
#     
#
from pylab import *
from numpy import ndarray, asarray, matrix, ones, hstack, linalg, isfinite

##########################
#
#  
#
##########################


def tall_and_skinny(A):
    return A.shape[0] > A.shape[1]

def prepend_ones_column(A):
    """
    Add a ones column to the left side of an array/matrix
    """
    ones_col = ones((A.shape[0], 1))
    return hstack((ones_col, A))

def linear_regression(X_input, Y_input):
    """
    Compute linear regression. Finds model, beta, that minimizes
    X*beta - Y in a least squared sense.

    Accepts inputs with type array
    Returns beta (type matrix), which is used only by apply_beta

    Example:
    >>> X = array([[5,2], [3,2], [6,2.1], [7, 3]]) # predictors
    >>> Y = array([5,2,6,6]) # depedent variable
    >>> beta = linear_regression(X, Y)  # compute the coefficients of the linear function
    >>> print beta
    matrix([[ 1.20104895]
            [ 1.41083916]
            [-1.6958042 ]])
    >>> print apply_beta(beta, X) # apply the function defined by beta, to each row of X
    array([ 4.86363636,
            2.04195804,
            6.1048951 ,
            5.98951049])
    """
    # Convert any input into tall and skinny matrices
    X = matrix(X_input)
    Y = matrix(Y_input)
    if not tall_and_skinny(X):
        X = X.T
    if not tall_and_skinny(Y):
        Y = Y.T

    if not isfinite(linalg.cond(X)):
        # matrix is not invertable
        return None

    X = prepend_ones_column(X)

    # Do actual computation
    beta = (X.T*X).I * X.T * Y
    return beta


def apply_beta(beta, Xs):
    '''
    Apply beta, the function generated by linear_regression, to the
    specified values

    Inputs:
      model: beta as returned by linear_regression
      Xs: 2D array of floats

    Returns:
      result of applying beta to the data, as an array.

      Given:
        beta = matrix([B0, B1, B2,...BN-1])
        Xs = array([[x01, x02, ..., x0N-1],
                    [x11, x12, ..., x1N-1],
                    ...
                    [xM1, xM2, ..., xMN-1]])

      result will be:
        array([B0+B1*x01+B2*x02+...+BN-1*x0N-1,
               B0+B1*x11+B2*x12+...+BN-1*x1N-1,
               ...
               B0+B1*xM1+B2*xM2+...+BN-1*xM-1])
    '''
    if not isinstance(Xs, ndarray):
        print "apply_model: Xs has wrong type.  Expected type ndarray.  Got " + str(type(Xs))
        return None

    if beta == None:
        print "beta is None"
        return None

    Xs = prepend_ones_column(Xs)
    YCALC = asarray(matrix(Xs)*beta)[:,0]
    return YCALC

def convert_if_float(v):
    '''
    If v is a floating point string, convert it to a floating point
    number and return the result.  Return v as is, otherwise.
    '''
    try:
        tmp = float(v)
        return tmp
    except ValueError:
        return v


def read_file(filename):
    '''
    Read data from the specified file.  Split the lines and convert
    float strings into floats.  Assumes the first row contains labels
    for the columns.

    Inputs:
      filename: name of the file to be read

    Returns:
      (list of strings, 2D array)
    '''
    data = [map(convert_if_float, x.strip().split(",")) for x in open(filename)]
    return (data[0], asarray(data[1:]))




######################
#
#   functions
#
######################


def compute_R2(beta, Xs, ys):
    '''
    Compute R-squared

    Inputs:
      beta: as computed by linear_regression
      Xs: 2D array of floats
      ys: 1D array floats

    Returns:
      float: R-squared value
    '''
    regs = apply_beta(beta, Xs)
    sum_deviations = sum((ys - mean(ys)) ** 2)
    sum_squares = sum( (regs - ys) ** 2 ) 
    fvu = sum_squares / sum_deviations
    r_square = 1 - fvu 

    return r_square


def mk_model(training_data, predictor_var_indices, dependent_var_index):
    '''
    Create a model for the dependent variable from the specified predictor variables

    Inputs:
      training_data: 2D array of floats
      predictor_var_indices: List of integer indices of the predictor variables
             ("columns") to include in model
      dependent_var_index: integer index of the dependent variable

    Returns:
       returns model (dictionary with values for "beta", "predictor_var_indices", "dependent_var_index", and "R2")
    '''

    model = {}
    (col_names, data) = read_file("data/city/training.csv")
    x_s = column_stack([data[:, i] for i in predictor_var_indices])
    y_s = data[:, dependent_var_index]
    beta = linear_regression(x_s, y_s)
    reg = compute_R2(beta, x_s, y_s)
    model["beta"] = beta
    model["predictor_var_indices"] = predictor_var_indices
    model["dependent_var_index"] = dependent_var_index
    model["R2"] = reg
    return model



def compute_best_bivariate(training_data, predictor_var_indices, dependent_var_index):
    '''
    Determine the best bivariable model for predicting the dependent variable using two
    predictor variables.

    Inputs:
      training_data: 2D array of floats
      predictor_var_indices: List of integer indices of the predictor variables
             ("columns") to include in model
      dependent_var_index: integer index of the dependent variable

    Returns: model
    '''
    best = {"R2": 0}
    for i in predictor_var_indices:
      for j in predictor_var_indices:
        if i != j:
          model = mk_model(training_data, [i, j], dependent_var_index)
          if model["R2"] > best["R2"]:
            best = model
    return best  




def discover_best_k_model(K, training_data, predictor_var_indices, dependent_var_index):
    '''
    Determine which K columns in predictor_var_indices yield
    the best model.

    Inputs:
      K: the number of predictor variables to include in the model
      training_data: 2D array of floats
      predictor_var_indices: List of integer indices of the predictor variables
             ("columns") to include in model
      dependent_var_index: integer index of the dependent variable
    Returns: model
    '''
    best = {"R2": 0}
    predictor_set = []
    for i in range(K):
      tracker = -1
      for x in predictor_var_indices:
        if x not in predictor_set:
          model = mk_model(training_data, predictor_set + [x], dependent_var_index)
          if model["R2"] > best["R2"]:
            tracker = x
            best = model
            # print model
      predictor_set.append(tracker)
    return best


    

def discover_best_model_with_threshold(training_data, predictor_var_indices,
                                       dependent_var_index, threshold):
    '''
    Determine best model for predicting the ys from a subset of the Xs,
    using the specified threshold to determine when to stop increasing
    the number of variables in the model

    Inputs:
      training_data: 2D array of floats
      predictor_var_indices: List of integer indices of the predictor variables
             ("columns") to include in model
      dependent_var_index: integer index of the dependent variable
      threshold: float

    Returns: model
    '''
    best = {"R2": 0}
    predictor_set = []
    for i in range(len(predictor_var_indices)):
      tracker = -1
      for x in predictor_var_indices:
        if x not in predictor_set:
          model = mk_model(training_data, predictor_set + [x], dependent_var_index)
          if model["R2"] > best["R2"]:
            if (model["R2"] - best["R2"]) > threshold:
              tracker = x
              best = model
            else:
              return best
      predictor_set.append(tracker)
    return best
    
